{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "To run in detached mode, open a `screen` and launch:\n",
    "```\n",
    "jupyter nbconvert --execute --inplace --to notebook VIB.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dalai/GNN_E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dalai/.conda/envs/gat_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/dalai/GNN_E\")\n",
    "print(os.getcwd())\n",
    "\n",
    "from scripts.models import *\n",
    "import pandas as pd\n",
    "from scripts.utils_models import *\n",
    "from math import ceil\n",
    "import gc\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(num_classes=13, type_labels='single', batch_size=32, test_batch_size=32, percentage_train=0.8, percentage_val=0.0, test_train_splitting_mode='Vertical', window_half_size=4, node_feat='symmetricwindow', initial_adj_method='clique', FN='Limbic', dataset_name='EMOTION', backbone='GCN', hidden_dim=16, num_layers=2, graph_type='prob', top_k=10, epsilon=0.3, graph_metric_type='mlp', num_per=16, feature_denoise=False, repar=True, beta=0.001, IB_size=16, graph_skip_conn=0.0, graph_include_self=True, folds=10, epochs=10, lr=0.0001, lr_decay_factor=0.5, lr_decay_step_size=50, weight_decay=5e-05)\n"
     ]
    }
   ],
   "source": [
    "# from VIB param_parser.py\n",
    "args = SimpleNamespace(\n",
    "\n",
    "    ### DATASET PARAMETERS\n",
    "    num_classes=13, #number of emotions to predict\n",
    "    type_labels=\"single\", #how the labels are encoded (single value, multi class one hot found with thr, ...)\n",
    "    batch_size=32,\n",
    "    test_batch_size=32,\n",
    "    percentage_train=0.8,\n",
    "    percentage_val=0.0,\n",
    "    test_train_splitting_mode=\"Vertical\", #How to split between train and test\n",
    "        # Vertical\n",
    "        # Horizontal\n",
    "    window_half_size=4, # Size of the window to use to create initial feautures\n",
    "    node_feat=\"symmetricwindow\", #how to create the node feautres\n",
    "    initial_adj_method=\"clique\", # hwo to initialize adge attr and edge connections\n",
    "        # Clique\n",
    "        # ...\n",
    "    FN=\"Limbic\", #functional method in case the iniial grpah is a subset of nodes\n",
    "    \n",
    "    ### VIB PARAMETERS\n",
    "    dataset_name=\"EMOTION\",\n",
    "    backbone=\"GCN\", # After graph leaning, VIB uses this backbone to precit the mu and std vectors\n",
    "        # GAT, GIN, GCN\n",
    "    hidden_dim=16, #hidden dim of the backbone\n",
    "    num_layers=2, #number layer in case backbon eins GIN\n",
    "    graph_type=\"prob\", #how the new graph is learnt\n",
    "        # epsiloNN --> Nodes are connected if their similarity (or attention score) is greater than epsilon.\n",
    "        # prob -->  probabilistically, where the edges are determined by a Bernoulli distribution parameterized by the attention scores.\n",
    "        # KNN --> each node to its k nearest neighbors based on the attention or similarity scores.\n",
    "    top_k=10, # in case the graph is learnt with KNN\n",
    "    epsilon=0.3, # in case the graph is elant using threhsolding\n",
    "    graph_metric_type=\"mlp\", #how to calculte similary between nodes in strucutre alening\n",
    "        # attention\n",
    "        # weighted_cosine --> cosine similarity but with learnable weights for each feature dimension\n",
    "        # cosine --> raw cosine similarity\n",
    "        # gat_attention --> graph attention mechanism inspired by Graph Attention Networks (GAT), using pairwise scores with a leaky ReLU activation\n",
    "        # kernel --> Gaussian kernel with learnable precision to compute distances\n",
    "        # transformer -->  Transformer-style attention, using query-key dot products scaled by feature dimensionality for similarity computation.\n",
    "        # mlp --> multi-layer perceptron (MLP) to transform features and compute pairwise similarity.\n",
    "        # multi-mlp\n",
    "    num_per=16, # how many perpsctive use for graph_metric_type, ex if gat_attention (how mnay heads), if multi-mlp (how mnay indepdnet mlp)\n",
    "    feature_denoise = False, \n",
    "        #enables or disables feature denoising during graph learning.\n",
    "        # masl useless feaures\n",
    "        # the mask is learnt\n",
    "        # if true only a subsampel of feat are used for graph learning\n",
    "    repar=True,\n",
    "    beta=0.001,# Weighting factor for the KL divergence in the VIB loss.\n",
    "        #High beta: Enforces a more compact latent representation, which can lead to better generalization by removing noise but may hurt task accuracy if too much useful information is discarded.\n",
    "        #Low beta: Retains more information in the latent representation, which may improve accuracy but risks overfitting or encoding noise.\n",
    "    IB_size=16, # emb dimension (i.e lenght of mu and std) (nb the last layer of gnn is double this value)\n",
    "    graph_skip_conn=0.0, # between [0-1], The new adjacency matrix is a linear combination of the initial adjacency matrix and the learned adjacency matrix\n",
    "    graph_include_self=True, # add self loops in new adj if graph_skip_conn==0\n",
    "\n",
    "    ### VIB TRAINING\n",
    "    folds=10,\n",
    "    epochs=10,\n",
    "    lr=0.0001,\n",
    "    lr_decay_factor=0.5,\n",
    "    lr_decay_step_size=50,\n",
    "    weight_decay=5e-5,\n",
    ")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load df all Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all movies with labels csv\n",
    "df_all_movies = pd.read_csv(f\"data/processed/all_movies_labelled_{args.num_classes}_{args.type_labels}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#JUST FOR PROVA: select subset of movies\n",
    "df_all_movies = df_all_movies[df_all_movies.movie.isin([0,3])]\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Vertical...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Splitting {args.test_train_splitting_mode}...\")\n",
    "\n",
    "if args.test_train_splitting_mode == \"Vertical\":\n",
    "    df_train, df_test = split_train_test_vertically(\n",
    "        df_all_movies, \n",
    "        test_movies_dict = {\"BigBuckBunny\": 2, \"FirstBite\": 4, \"Superhero\": 9}\n",
    "    )\n",
    "    df_val = df_train[df_train.id == 99] #make sure to be empty\n",
    "elif args.test_train_splitting_mode == \"Horizontal\":\n",
    "    df_train, df_val, df_test = split_train_val_test_horizontally(\n",
    "        df_all_movies, \n",
    "        percentage_train=args.percentage_train, \n",
    "        percentage_val=args.percentage_val, #0 to not have nay val set\n",
    "        path_pickle_delay=\"data/raw/labels/run_onsets.pkl\",\n",
    "        path_movie_title_mapping=\"data/raw/labels/category_mapping_movies.csv\", \n",
    "        tr_len=1.3\n",
    "    )\n",
    "elif args.test_train_splitting_mode == \"MovieRest\":\n",
    "    df_rest = pd.read_csv(\"data/raw/rest/Rest_compiled414_processed.csv\")\n",
    "    df_train, df_test = split_train_test_rest_classification(df_all_movies, df_rest)\n",
    "    df_val = df_train[df_train.id == 99] #make sure to be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset (i.e. graph list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DatasetEmo(\n",
    "    df = df_train, #df with mvoies to use\n",
    "    node_feat = args.node_feat, #\"singlefmri\", \"symmetricwindow\", \"pastwindow\"\n",
    "    initial_adj_method = args.initial_adj_method,\n",
    "        # \"clique\"\n",
    "        #FC dynamic:  \"fcmovie\", \"fcwindow\"\n",
    "        #FN (subcorticla with clique): \"FN_const\" \"FN_edgeAttr_FC_window\" \"FN_edgeAttr_FC_movie\"\n",
    "    FN = args.FN, #['Vis' 'SomMot' 'DorsAttn' 'SalVentAttn' 'Limbic' 'Cont' 'Default' 'Sub']\n",
    "    FN_paths = \"data/raw/FN_raw\",\n",
    "    sizewind = args.window_half_size\n",
    ")\n",
    "\n",
    "dataset_val = DatasetEmo(\n",
    "    df = df_val,\n",
    "    node_feat = args.node_feat,\n",
    "    initial_adj_method = args.initial_adj_method,\n",
    "    FN = args.FN,\n",
    "    FN_paths = \"data/raw/FN_raw\",\n",
    "    sizewind = args.window_half_size\n",
    ")\n",
    "\n",
    "dataset_test = DatasetEmo(\n",
    "    df = df_test,\n",
    "    node_feat = args.node_feat,\n",
    "    initial_adj_method = args.initial_adj_method,\n",
    "    FN = args.FN,\n",
    "    FN_paths = \"data/raw/FN_raw\",\n",
    "    sizewind = args.window_half_size\n",
    ")\n",
    "\n",
    "# Extarct the list of graphs of each dataset\n",
    "graphs_list_train = dataset_train.get_graphs_list()\n",
    "graphs_list_val = dataset_val.get_graphs_list()\n",
    "graphs_list_test = dataset_test.get_graphs_list()\n",
    "\n",
    "print()\n",
    "print(f\"Number Batces Train {len(graphs_list_train)/args.batch_size}\")\n",
    "print(f\"Number Batces Val {len(graphs_list_val)/args.batch_size}\")\n",
    "print(f\"Number Batces Test {len(graphs_list_test)/args.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Istantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# following VIB main.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mVIB\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mVIB\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgsl\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgsl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Number fo features for each node\u001b[39;00m\n\u001b[1;32m      6\u001b[0m num_node_features \u001b[38;5;241m=\u001b[39m graphs_list_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/GNN_E/VIB/gsl.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# following VIB main.py\n",
    "import VIB.gsl as gsl\n",
    "\n",
    "# Number fo features for each node\n",
    "num_node_features = graphs_list_train[0].x.shape[1]\n",
    "print(\"\\nnum_node_features : %d, num_classes : %d\"%(num_node_features, args.num_classes))\n",
    "\n",
    "model = gsl.VIBGSL(\n",
    "            args, \n",
    "            num_node_features, \n",
    "            args.num_classes)\n",
    "print(model.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VIB.train_eval import *\n",
    "\n",
    "# Useful if the code get some strange anomaly\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_losses, train_accs, test_losses, test_accs = my_train_and_evaluate(\n",
    "    train_graphs_list = graphs_list_train,\n",
    "    test_graphs_list = graphs_list_test,\n",
    "    model = model,\n",
    "    epochs = args.epochs, \n",
    "    batch_size = args.batch_size, \n",
    "    test_batch_size = args.test_batch_size,\n",
    "    lr = args.lr, \n",
    "    lr_decay_factor = args.lr_decay_factor, \n",
    "    lr_decay_step_size = args.lr_decay_step_size,\n",
    "    weight_decay = args.weight_decay, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test\n",
    "Wite the last epoch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extarct accuracy, learnt grpahs and predicted lablled in test set\n",
    "acc_test, new_graphs_list_test, pred_y_test = my_interpretation(\n",
    "        graphs_list = graphs_list_test,\n",
    "        model_trained = model,\n",
    "        batch_size = args.batch_size,\n",
    ")\n",
    "\n",
    "# Extarct ground.truth labels\n",
    "pred_y_test = [y.item() for y in pred_y_test]\n",
    "y_test = [g.y.item() for g in graphs_list_test]\n",
    "\n",
    "# Create a dictionary where the keys are labels and the values are lists of adjacency matrices\n",
    "dict_new_graphs_list_test_adj = {}\n",
    "for g, label in zip(new_graphs_list_test, y_test):\n",
    "    label = str(label)  # Convert the label to string\n",
    "    adj = to_dense_adj(edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "    adj = adj.cpu().squeeze().numpy()  # Convert to numpy array\n",
    "    if label not in dict_new_graphs_list_test_adj:\n",
    "        dict_new_graphs_list_test_adj[label] = []  # Create a list for this label if it doesn't exist\n",
    "    dict_new_graphs_list_test_adj[label].append(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = Path(f\"data/results/VIB/{int(np.ceil(acc_test*100))}\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# Convert SimpleNamespace to dictionary\n",
    "results_dict = vars(args)\n",
    "# Create dict with all results\n",
    "results_dict.update(\n",
    "    {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accs\": test_accs, \n",
    "        \"acc_test\": acc_test,\n",
    "        \"pred_y_test\": pred_y_test, \n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    ")\n",
    "with open(os.path.join(RESULT_DIR, 'results.json'), 'w') as f:\n",
    "    json.dump(results_dict, f, indent=4)\n",
    "\n",
    "# Save test adk mayrices\n",
    "np.savez_compressed(os.path.join(RESULT_DIR, 'adj_test.npz'), **dict_new_graphs_list_test_adj, labels=y_test)\n",
    "\n",
    "# Save the entire model (architecture + weights)\n",
    "torch.save(model, os.path.join(RESULT_DIR, 'full_model.pth'))\n",
    "\n",
    "\n",
    "# For future Loading\n",
    "#model = torch.load('full_model.pth')\n",
    "#model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "data_from_npz = np.load(os.path.join(RESULT_DIR, 'adj_test.npz'))\n",
    "\n",
    "# Check the available keys\n",
    "print(data_from_npz.files)\n",
    "\n",
    "# Access the matrices grouped by label\n",
    "label = '5'  # Example label (you can loop through all or access specific labels)\n",
    "adj_list = data_from_npz[label]  # List of adjacency matrices for label '5'\n",
    "print(adj_list.shape)\n",
    "\n",
    "# Access the labels stored separately\n",
    "labels = data_from_npz['labels']\n",
    "print(labels.shape)  # Shape of the labels\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(adj_list[0].shape)  # Shape of the first adjacency matrix for this label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs_list_test_fear = [g for g in graphs_list_test if g.y == 5]\n",
    "# print(len(graphs_list_test_fear))\n",
    "# print(graphs_list_test_fear[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs_list, new_graphs_list, pred_y = my_interpretation(\n",
    "#         graphs_list = graphs_list_test_fear,\n",
    "#         model_trained = model,\n",
    "#         batch_size = args.batch_size,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "\n",
    "# # Assuming initial_graph and new_graph are returned from to_dense_adj and are PyTorch tensors\n",
    "# # Move tensors to CPU and convert to numpy arrays\n",
    "# initial_graph_np = initial_graph.cpu().squeeze().numpy()  # Move to CPU, remove singleton dimensions, and convert to numpy\n",
    "# new_graph_np = new_graph.cpu().squeeze().numpy()  # Same for new_graph\n",
    "\n",
    "# # Set up the matplotlib figure\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Create 2 subplots side by side\n",
    "\n",
    "# # Plot the initial graph adjacency matrix\n",
    "# sns.heatmap(initial_graph_np, cmap='Blues', ax=ax[0], square=True, cbar=True)\n",
    "# ax[0].set_title('Initial Graph Adjacency Matrix')\n",
    "# ax[0].set_xlabel('Nodes')\n",
    "# ax[0].set_ylabel('Nodes')\n",
    "\n",
    "# # Plot the new graph adjacency matrix\n",
    "# sns.heatmap(new_graph_np, cmap='Blues', ax=ax[1], square=True, cbar=True)\n",
    "# ax[1].set_title('New Graph Adjacency Matrix')\n",
    "# ax[1].set_xlabel('Nodes')\n",
    "# ax[1].set_ylabel('Nodes')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gat_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
